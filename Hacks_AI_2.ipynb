{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c383604f-abc8-469b-82ec-a5cb8e48088d",
   "metadata": {},
   "source": [
    "Здесь всё основное как бы. Используется кэш дир чтобы сохранять модельку локально, чтобы потом её не пришлось перекачивать заново и не тратить кучу времени в пустую."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31c182b1-48c9-44cf-9447-7fd9f355a03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\"openai/whisper-small\",cache_dir=\"C:/huggingface_cache/\", torch_dtype=torch_dtype, use_safetensors=True, use_cache=True)\n",
    "processor = AutoProcessor.from_pretrained(\"openai/whisper-small\",cache_dir=\"C:/huggingface_cache/\", use_cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cebe13-3f9d-4f9b-90c3-7a7f64363953",
   "metadata": {},
   "source": [
    "Получаем пайплайн, обязательно указываем модель и токенайзер с процессора который мы получили выше, без данного процесса ничего работать не будет уж точно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cbede72-8eb1-486f-aec9-7e7916217ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    return_timestamps=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bb6889-9859-4486-9185-c0dbd2803914",
   "metadata": {},
   "source": [
    "На выходе мы получаем полный текст и чанки с текстом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6aa21d34-b743-4079-8e71-fddab957c905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' В этом одном предложении Нрашин текст есть доля.',\n",
       " 'chunks': [{'timestamp': (0.0, 2.8),\n",
       "   'text': ' В этом одном предложении Нрашин текст есть доля.'}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"7.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597de11e-3c6a-4bda-834d-7efc01a4a48f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22ff43f5-0b95-4136-a762-7e805dc3fadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3357b31-140d-4f48-bdf9-465b6007d5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.downloader.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d509242-b1bd-4361-9802-c599af3b0a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae789e27-f0d3-4067-a0bd-4b56488fc0c7",
   "metadata": {},
   "source": [
    "Тут используется достаточно интересное решение, а точнее свод любого текста (не важно на каком языке) в английский. Таким образом мы получаем возможность обрабатывать тональность текста на абсолютно любом языке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca2505d2-762c-4441-8071-b2ef25c4fd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "#Используем гугл переводчик для перевода\n",
    "translated = GoogleTranslator(source='auto', target='en').translate(\"эа\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd978ab1-f532-4391-b990-edd85693220b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores(translated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3675a26-5b27-4155-bdc9-e8e33a44e2f9",
   "metadata": {},
   "source": [
    "# Дальше всё было упаковано в классы для удобного взаимодействия."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff8995a-aa1a-44c9-a17e-9285d4a15591",
   "metadata": {},
   "source": [
    "Готовый класс, при инициализации нужно указать путь до кэш папки (чтобы модель не перекачивалась заново), у класса есть функция predict tag которая должна выявлять разметку: Highlights, base, 18+, gray, black, но пока она определяет base и black. Есть возможность её расширить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f3910e7-4f42-4bd9-aa4a-77200975ac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tagClassModel:\n",
    "    def __init__(self, cache_dir):\n",
    "        from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\"KoalaAI/Text-Moderation\",cache_dir=cache_dir)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"KoalaAI/Text-Moderation\",cache_dir=cache_dir)\n",
    "    def predict_tag(self, text):\n",
    "        from deep_translator import GoogleTranslator\n",
    "        translated = GoogleTranslator(source='auto', target='en').translate(text)\n",
    "        inputs = self.tokenizer(translated, return_tensors=\"pt\")\n",
    "        outputs = self.model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = logits.softmax(dim=-1).squeeze()\n",
    "        id2label = self.model.config.id2label\n",
    "        labels = [id2label[idx] for idx in range(len(probabilities))]\n",
    "        label_prob_pairs = list(zip(labels, probabilities))\n",
    "        label_prob_pairs.sort(key=lambda item: item[1], reverse=True)\n",
    "        ok = 0\n",
    "        sh = 0\n",
    "        for label, probability in label_prob_pairs:\n",
    "            if label == \"SH\":\n",
    "                sh = float(probability)\n",
    "            elif label == \"OK\":\n",
    "                ok = float(probability)\n",
    "        if sh > ok:\n",
    "            return \"Black\"\n",
    "        else:\n",
    "            return \"Base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a01744f-aae4-470b-a271-393f2a855e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "747a20b7700a407cbf2cc653c17e4d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.13k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\huggingface_cache\\models--KoalaAI--Text-Moderation. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd15301a10d40c19aaec0fbc19763ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/557M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf088857f8e4dad9108236570dce329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a319d0c47b24a7a841c3a49cf2c873d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3268590c76494e20af006099325983aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49eb93029797476c8362e17c329d50e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ad116b381d949ff82d963da5bb82f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/963 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tcm = tagClassModel(\"C:/huggingface_cache/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6374506f-1047-4fc8-af9c-38ff8b3ec9cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Base'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcm.predict_tag(\"Жизнь прекрасна\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3db6639-fcd4-41c5-a074-61a4ca8c7c5c",
   "metadata": {},
   "source": [
    "Тональность текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d585e076-01e8-4831-a0b9-7aece726c55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTonality(text):\n",
    "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "    from tqdm.notebook import tqdm\n",
    "    import nltk\n",
    "    \n",
    "    nltk.downloader.download('vader_lexicon')\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    from deep_translator import GoogleTranslator\n",
    "    translated = GoogleTranslator(source='auto', target='en').translate(text)\n",
    "    return sia.polarity_scores(translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8639df9c-93d0-4ead-831d-a27543d84177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getTonality(\"тест тональности\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d317b38-6ffe-4844-a051-34980e3538dc",
   "metadata": {},
   "source": [
    "Транскрибация текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "976d40e2-5b1c-41c0-8cdf-b1624dddb14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transcribation:\n",
    "    def __init__(self, cache_dir):\n",
    "        import torch\n",
    "        from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "        from datasets import load_dataset\n",
    "\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "        self.model = AutoModelForSpeechSeq2Seq.from_pretrained(\"openai/whisper-small\",cache_dir=cache_dir, torch_dtype=torch_dtype, use_safetensors=True, use_cache=True)\n",
    "        self.processor = AutoProcessor.from_pretrained(\"openai/whisper-small\",cache_dir=cache_dir, use_cache=True)\n",
    "        self.pipe = pipeline(\n",
    "            \"automatic-speech-recognition\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.processor.tokenizer,\n",
    "            feature_extractor=self.processor.feature_extractor,\n",
    "            return_timestamps=True,\n",
    "            torch_dtype=torch_dtype,\n",
    "            device=device,\n",
    "        )\n",
    "    def audio2text(self, audio_path):\n",
    "        return self.pipe(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8b29692f-9da8-4de7-8d2d-233eb7e64e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = Transcribation(\"C:/huggingface_cache/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e172201d-8a7a-44ac-a8dd-6c44681a81bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' тут тест на русском. Here is very strong text in English all right.',\n",
       " 'chunks': [{'timestamp': (0.0, 1.0), 'text': ' тут тест на русском.'},\n",
       "  {'timestamp': (1.0, 3.8),\n",
       "   'text': ' Here is very strong text in English all right.'}]}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc.audio2text(\"4.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6dfce9-fb84-4c51-a498-cf39f60e8bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
